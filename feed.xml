<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://galobelwang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://galobelwang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-28T11:29:24+00:00</updated><id>https://galobelwang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Information and Coding theory, Algebraic geometry and Hodge theory, Algebraic combinatorics etc. </subtitle><entry><title type="html">A brief introduction to differential geometry</title><link href="https://galobelwang.github.io/blog/2025/axiomatization-of-probability/" rel="alternate" type="text/html" title="A brief introduction to differential geometry"/><published>2025-08-28T00:00:00+00:00</published><updated>2025-08-28T00:00:00+00:00</updated><id>https://galobelwang.github.io/blog/2025/axiomatization-of-probability</id><content type="html" xml:base="https://galobelwang.github.io/blog/2025/axiomatization-of-probability/"><![CDATA[<h1 id="smooth-manifolds-and-tangent-spaces">Smooth Manifolds and Tangent Spaces</h1> <h3 id="introduction">Introduction</h3> <p>Welcome to the first part of our crash course on Differential Geometry for Machine Learning! In this post, we’ll explore the fundamental concept of a <strong>smooth manifold</strong>. Think of manifolds as generalized surfaces – spaces that locally “look like” familiar Euclidean space (e.g., \(\mathbb{R}^n\)) but can have a more complex global structure.</p> <p>Why are manifolds important in machine learning?</p> <ul> <li>The <strong>parameter space</strong> of many machine learning models (like neural networks) can be viewed as a high-dimensional manifold.</li> <li>The <strong>loss landscape</strong>, which we navigate during optimization, is often a function defined over such a manifold.</li> <li>Some models have <strong>constraints</strong> on their parameters (e.g., weights of an autoencoder forming a low-dimensional representation, orthogonal matrices in certain RNNs) which naturally define manifolds.</li> </ul> <p>Our goal here is to build intuition for what manifolds are and introduce <strong>tangent spaces</strong>, which are crucial for understanding concepts like gradients in these curved settings.</p> <h3 id="beyond-euclidean-space-the-need-for-manifolds">Beyond Euclidean Space: The Need for Manifolds</h3> <p>In basic calculus and linear algebra, we often work within Euclidean spaces like \(\mathbb{R}^2\) (the plane) or \(\mathbb{R}^3\) (3D space). These spaces are “flat” and have a global coordinate system. However, many interesting spaces are not globally flat.</p> <p>Consider the surface of a sphere, \(S^2\). Locally, any small patch on the sphere looks like a piece of the flat plane \(\mathbb{R}^2\). But globally, you can’t map the entire sphere to a single flat plane without distortion (think of world maps). The sphere is a simple example of a manifold.</p> <p>In machine learning:</p> <ul> <li>The set of all probability distributions of a certain type (e.g., all Gaussian distributions) forms a manifold. The parameters (mean and covariance) live in this space.</li> <li>The set of weight matrices for a neural network layer, perhaps with some normalization constraints (e.g., weights on a sphere, orthogonal matrices), can form a manifold.</li> <li>The loss function of a neural network is a function \(L: \mathcal{W} \to \mathbb{R}\), where \(\mathcal{W}\) is the space of all possible weights. This space \(\mathcal{W}\) is typically a very high-dimensional manifold (often just \(\mathbb{R}^N\) for unconstrained networks, but its geometry under a suitable metric, like the Fisher Information Metric, can be non-trivial).</li> </ul> <p>Differential geometry provides the tools to perform calculus on these more general spaces.</p> <h3 id="what-is-a-smooth-manifold">What is a Smooth Manifold?</h3> <p>Intuitively, an \(n\)-dimensional manifold is a space that, if you “zoom in” enough at any point, looks like an open subset of \(\mathbb{R}^n\).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> An <strong>\(n\)-dimensional topological manifold</strong> \(M\)</p> </div> <p>A topological space \(M\) is an \(n\)-dimensional topological manifold if:</p> <ol> <li>\(M\) is <strong>Hausdorff</strong>: Any two distinct points have disjoint open neighborhoods.</li> <li>\(M\) is <strong>second-countable</strong>: \(M\) has a countable basis for its topology. (These ensure \(M\) is “nice” enough.)</li> <li>\(M\) is <strong>locally Euclidean of dimension \(n\)</strong>: Every point \(p \in M\) has an open neighborhood \(U\) that is homeomorphic to an open subset \(V \subseteq \mathbb{R}^n\). A homeomorphism is a continuous bijection with a continuous inverse.</li> </ol> <p>The pair \((U, \phi)\), where \(\phi: U \to V \subseteq \mathbb{R}^n\) is such a homeomorphism, is called a <strong>chart</strong> (or coordinate system) around \(p\). The functions \(x^i = \pi^i \circ \phi\) (where \(\pi^i\) are projections onto coordinate axes in \(\mathbb{R}^n\)) are local coordinate functions.</p> </blockquote> <details class="details-block"> <summary> <p><strong>Analogy.</strong> Chart on Earth</p> </summary> <p>Think of a map of a small region on Earth (e.g., a city map). This map is a chart. It represents a piece of the curved surface of the Earth on a flat piece of paper (\(\mathbb{R}^2\)). You need many such maps (an atlas) to cover the entire Earth, and where they overlap, they must be consistent.</p> </details> <p>For calculus, we need more than just a topological manifold; we need a <strong>smooth manifold</strong>. This means that when two charts overlap, the transition from one set of coordinates to another must be smooth (infinitely differentiable, \(C^\infty\)).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Smooth Atlas and Smooth Manifold</strong></p> </div> <p>Let \(M\) be an \(n\)-dimensional topological manifold.</p> <ol> <li> <p>Two charts \((U_\alpha, \phi_\alpha)\) and \((U_\beta, \phi_\beta)\) are <strong>smoothly compatible</strong> if \(U_\alpha \cap U_\beta = \emptyset\), or the <strong>transition map</strong></p> \[\psi_{\beta\alpha} = \phi_\beta \circ \phi_\alpha^{-1} : \phi_\alpha(U_\alpha \cap U_\beta) \to \phi_\beta(U_\alpha \cap U_\beta)\] <p>is a diffeomorphism (a smooth map with a smooth inverse). Both \(\phi_\alpha(U_\alpha \cap U_\beta)\) and \(\phi_\beta(U_\alpha \cap U_\beta)\) are open subsets of \(\mathbb{R}^n\).</p> </li> <li>An <strong>atlas</strong> for \(M\) is a collection of charts \(\mathcal{A} = \{(U_\alpha, \phi_\alpha)\}\) such that \(\bigcup_\alpha U_\alpha = M\).</li> <li>A <strong>smooth atlas</strong> is an atlas whose charts are all pairwise smoothly compatible.</li> <li>A <strong>smooth structure</strong> on \(M\) is a maximal smooth atlas (one that contains every chart compatible with it, and any two charts in it are smoothly compatible).</li> <li>A <strong>smooth manifold</strong> (or differentiable manifold) is a topological manifold equipped with a smooth structure.</li> </ol> </blockquote> <p>The key idea is that we can do calculus locally within each chart using standard multivariable calculus, and the smoothness of transition maps ensures that these local calculations are consistent across different charts.</p> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> The Circle \(S^1\)</p> </div> <p>The unit circle \(S^1 = \{(x, y) \in \mathbb{R}^2 \vert x^2 + y^2 = 1\}\) is a 1-dimensional manifold. We need at least two charts to cover \(S^1\). A common way is using angular parameterizations:</p> <ul> <li>Chart 1: Let \(U_1 = S^1 \setminus \{(1,0)\}\) (circle minus the point \((1,0)\)). Define \(\phi_1: U_1 \to (0, 2\pi)\) by \(\phi_1(\cos\theta, \sin\theta) = \theta\).</li> <li>Chart 2: Let \(U_2 = S^1 \setminus \{(-1,0)\}\) (circle minus the point \((-1,0)\)). Define \(\phi_2: U_2 \to (-\pi, \pi)\) by \(\phi_2(\cos\theta, \sin\theta) = \theta\).</li> </ul> <p>Consider the overlap. For instance, take the upper semi-circle, corresponding to \(\theta \in (0, \pi)\). Points here are in both \(U_1\) and \(U_2\). \(\phi_1(U_1 \cap U_2) = (0, \pi) \cup (\pi, 2\pi)\). \(\phi_2(U_1 \cap U_2) = (-\pi, 0) \cup (0, \pi)\).</p> <p>Let \(p \in U_1 \cap U_2\). If \(\phi_1(p) = \theta_1 \in (0, \pi)\), then \(\phi_2(p) = \theta_1\). The transition map \(\phi_2 \circ \phi_1^{-1}(\theta_1) = \theta_1\). If \(\phi_1(p) = \theta_1 \in (\pi, 2\pi)\) (e.g., lower semi-circle), then \(\phi_2(p) = \theta_1 - 2\pi\). The transition map \(\phi_2 \circ \phi_1^{-1}(\theta_1) = \theta_1 - 2\pi\). These transition functions are smooth (linear, in fact). (Another common way to define charts for spheres is using stereographic projection).</p> </blockquote> <p>Other examples of smooth manifolds:</p> <ul> <li>Any open subset of \(\mathbb{R}^n\) is an \(n\)-manifold (with a single chart: the identity map).</li> <li>The sphere \(S^n = \{x \in \mathbb{R}^{n+1} \mid \Vert x \Vert = 1\}\).</li> <li>The torus \(T^n = S^1 \times \dots \times S^1\) (\(n\) times).</li> <li>The space of \(m \times n\) matrices, \(\mathbb{R}^{m \times n}\), which is just Euclidean space.</li> <li><strong>Lie groups:</strong> Manifolds with a compatible group structure. Examples: <ul> <li>\(GL(n, \mathbb{R})\): invertible \(n \times n\) real matrices.</li> <li>\(O(n)\): orthogonal \(n \times n\) matrices (\(A^T A = I\)).</li> <li>\(SO(n)\): special orthogonal matrices (\(A^T A = I, \det A = 1\)).</li> <li>These are crucial in physics and also appear in ML (e.g., orthogonal RNNs, parameterizing rotations).</li> </ul> </li> </ul> <h5 id="smooth-functions-on-manifolds">Smooth Functions on Manifolds</h5> <p>A function \(f: M \to \mathbb{R}\) is <strong>smooth</strong> if for every chart \((U, \phi)\) on \(M\), the composite function \(f \circ \phi^{-1}: \phi(U) \to \mathbb{R}\) is smooth in the usual sense of multivariable calculus (i.e., it has continuous partial derivatives of all orders on the open set \(\phi(U) \subseteq \mathbb{R}^n\)). Similarly, a map \(F: M \to N\) between two smooth manifolds is smooth if its representation in local coordinates is smooth. Loss functions in ML are typically assumed to be smooth (or at least twice differentiable) on the parameter manifold.</p> <h3 id="tangent-vectors-and-tangent-spaces">Tangent Vectors and Tangent Spaces</h3> <p>Now that we have a notion of a smooth manifold, we want to do calculus. The first step is to define derivatives. On a manifold, derivatives are captured by <strong>tangent vectors</strong>.</p> <p>Intuitively, a tangent vector at a point \(p \in M\) is a vector that “points along” a curve passing through \(p\), representing the instantaneous velocity of the curve.</p> <p>There are several equivalent ways to define tangent vectors:</p> <h5 id="a-equivalence-classes-of-curves-intuitive">a) Equivalence Classes of Curves (Intuitive)</h5> <p>A smooth curve through \(p \in M\) is a smooth map \(\gamma: (-\epsilon, \epsilon) \to M\) such that \(\gamma(0) = p\). Two curves \(\gamma_1, \gamma_2\) through \(p\) are considered equivalent if their representations in any local chart \((U, \phi)\) around \(p\) have the same derivative (velocity vector in \(\mathbb{R}^n\)) at \(t=0\):</p> \[\frac{d}{dt}(\phi \circ \gamma_1)(t) \Big\vert_{t=0} = \frac{d}{dt}(\phi \circ \gamma_2)(t) \Big\vert_{t=0}\] <p>A tangent vector at \(p\) is an equivalence class of such curves. The set of all tangent vectors at \(p\) is the <strong>tangent space</strong> \(T_p M\). This space can be shown to have the structure of an \(n\)-dimensional vector space.</p> <h5 id="b-derivations-abstract-and-powerful">b) Derivations (Abstract and Powerful)</h5> <p>A <strong>derivation</strong> at a point \(p \in M\) is a linear map \(v: C^\infty(M) \to \mathbb{R}\) (where \(C^\infty(M)\) is the space of smooth real-valued functions on \(M\)) satisfying the Leibniz rule (product rule):</p> \[v(fg) = f(p)v(g) + g(p)v(f) \quad \text{for all } f, g \in C^\infty(M)\] <p>It can be shown that the set of all derivations at \(p\) forms an \(n\)-dimensional vector space, and this vector space is isomorphic to the tangent space defined via curves. This is often taken as the formal definition of \(T_p M\).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Tangent Space \(T_p M\)</strong></p> </div> <p>The <strong>tangent space</strong> to a smooth manifold \(M\) at a point \(p \in M\), denoted \(T_p M\), is the vector space of all derivations at \(p\). An element \(v \in T_p M\) is called a <strong>tangent vector</strong> at \(p\). If \(M\) is an \(n\)-dimensional manifold, then \(T_p M\) is an \(n\)-dimensional real vector space.</p> </blockquote> <p>Given a chart \((U, \phi)\) with local coordinates \((x^1, \dots, x^n)\) around \(p\), a natural basis for \(T_p M\) is given by the partial derivative operators with respect to these coordinates, evaluated at \(p\):</p> \[\left\{ \frac{\partial}{\partial x^1}\Big\vert_p, \dots, \frac{\partial}{\partial x^n}\Big\vert_p \right\}\] <p>Here, \(\frac{\partial}{\partial x^i}\Big\vert_p\) is the derivation that acts on a function \(f \in C^\infty(M)\) as:</p> \[\left(\frac{\partial}{\partial x^i}\Big\vert_p\right)(f) := \frac{\partial (f \circ \phi^{-1})}{\partial u^i} \Big\vert_{\phi(p)}\] <p>where \((u^1, \dots, u^n)\) are the standard coordinates on \(\mathbb{R}^n\) corresponding to \(\phi(U)\). Any tangent vector \(v \in T_p M\) can be written uniquely as a linear combination of these basis vectors:</p> \[v = \sum_{i=1}^n v^i \frac{\partial}{\partial x^i}\Big\vert_p\] <p>The coefficients \(v^i\) are the <strong>components</strong> of the vector \(v\) in the coordinate basis \(\{\partial/\partial x^i\vert_p\}\).</p> <blockquote class="box-info"> <p><strong>Connection to Gradients in ML.</strong> In Euclidean space \(\mathbb{R}^n\), the gradient \(\nabla f(p)\) of a function \(f\) at \(p\) is a vector. If we consider a path \(\gamma(t)\) with \(\gamma(0)=p\) and velocity \(\gamma'(0) = v\), then the directional derivative of \(f\) along \(v\) is \(D_v f(p) = \nabla f(p) \cdot v\). On a manifold, the concept analogous to the gradient is related to the <strong>differential</strong> \(df_p\) of a function \(f: M \to \mathbb{R}\). This differential \(df_p\) is an element of the <em>cotangent space</em> \(T_p^\ast M\) (the dual space of \(T_p M\)). It acts on tangent vectors: \(df_p(v) = v(f)\). If the manifold has a Riemannian metric (Part 2), there’s a natural way to identify tangent vectors with cotangent vectors. This allows us to define a <strong>gradient vector field</strong> \(\text{grad } f\) (or \(\nabla f\)) which is a tangent vector field. Its components in a local coordinate system are related to the partial derivatives \(\partial f / \partial x^i\). For now, think of tangent vectors as the “directions” in which one can move from \(p\), and \(T_p M\) is the space where these directions (and eventually gradients) live.</p> </blockquote> <h3 id="the-differential-pushforward-of-a-smooth-map">The Differential (Pushforward) of a Smooth Map</h3> <p>If we have a smooth map \(F: M \to N\) between two smooth manifolds, it induces a linear map between their tangent spaces at corresponding points.</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Differential (or Pushforward)</strong></p> </div> <p>Let \(F: M \to N\) be a smooth map between smooth manifolds. For any point \(p \in M\), the <strong>differential</strong> of \(F\) at \(p\) (also called the <strong>pushforward</strong> by \(F\) at \(p\)) is the linear map:</p> \[(F_\ast )_p : T_p M \to T_{F(p)} N\] <p>(also denoted \(dF_p\) or \(DF(p)\)) defined as follows: for any tangent vector \(v \in T_p M\) (viewed as a derivation) and any smooth function \(g \in C^\infty(N)\),</p> \[((F_\ast )_p v)(g) := v(g \circ F)\] <p>The function \(g \circ F\) is a smooth function on \(M\), so \(v(g \circ F)\) is well-defined. Alternatively, if \(v \in T_p M\) is represented by a curve \(\gamma: (-\epsilon, \epsilon) \to M\) with \(\gamma(0)=p\) and \(\gamma'(0)=v\), then \((F_\ast )_p v\) is the tangent vector at \(F(p) \in N\) represented by the curve \(F \circ \gamma: (-\epsilon, \epsilon) \to N\). That is, \((F_\ast )_p(\gamma'(0)) = (F \circ \gamma)'(0)\).</p> </blockquote> <p>In local coordinates, let \(M\) have coordinates \((x^1, \dots, x^m)\) near \(p\) and \(N\) have coordinates \((y^1, \dots, y^n)\) near \(F(p)\). If \(F\) is represented by coordinate functions \(y^j = F^j(x^1, \dots, x^m)\), then the matrix representation of \((F_\ast )_p\) with respect to the coordinate bases \(\{\partial/\partial x^i\vert_p\}\) and \(\{\partial/\partial y^j\vert_{F(p)}\}\) is the <strong>Jacobian matrix</strong> of \(F\) at \(p\):</p> \[[ (F_\ast )_p ]^j_i = \frac{\partial F^j}{\partial x^i} \Big\vert_p\] <p>So, if \(v = \sum_i v^i \frac{\partial}{\partial x^i}\Big\vert_p\), then \((F_\ast )_p v = w = \sum_j w^j \frac{\partial}{\partial y^j}\Big\vert_{F(p)}\), where</p> \[w^j = \sum_{i=1}^m \left( \frac{\partial F^j}{\partial x^i} \Big\vert_p \right) v^i\] <h3 id="vector-fields">Vector Fields</h3> <p>A <strong>smooth vector field</strong> \(X\) on a manifold \(M\) is a smooth assignment of a tangent vector \(X_p \in T_p M\) to each point \(p \in M\). “Smooth” here means that if we express \(X\) in any local coordinate system \((x^1, \dots, x^n)\) as</p> \[X(p) = \sum_{i=1}^n X^i(p) \frac{\partial}{\partial x^i}\Big\vert_p\] <p>then the component functions \(X^i: U \to \mathbb{R}\) are smooth functions on the chart’s domain \(U\). Equivalently, a vector field \(X\) is smooth if for every smooth function \(f \in C^\infty(M)\), the function \(p \mapsto X_p(f)\) (which can be written as \((Xf)(p)\)) is also a smooth function on \(M\).</p> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> Gradient Fields in Optimization</p> </div> <p>If \(M = \mathbb{R}^n\) (a trivial manifold), and \(L: \mathbb{R}^n \to \mathbb{R}\) is a smooth loss function, its gradient</p> \[\nabla L(p) = \left( \frac{\partial L}{\partial x^1}(p), \dots, \frac{\partial L}{\partial x^n}(p) \right)\] <p>is typically identified with the vector field</p> \[X_L(p) = \sum_{i=1}^n \frac{\partial L}{\partial x^i}(p) \frac{\partial}{\partial x^i}\Big\vert_p\] <p>Gradient descent involves taking steps in the direction of \(-X_L(p)\). More generally, on a Riemannian manifold (which we’ll introduce later), the gradient vector field \(\nabla L\) is intrinsically defined. Optimization algorithms often aim to follow trajectories of such (or related) vector fields to find minima of \(L\).</p> </blockquote>]]></content><author><name>Galobel Wang</name></author><category term="geometry"/><summary type="html"><![CDATA[Foundational preliminaries of information geometry]]></summary></entry></feed>