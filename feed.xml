<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://galobelwang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://galobelwang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-09T10:44:06+00:00</updated><id>https://galobelwang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Information and Coding theory, Algebraic geometry and Hodge theory, Algebraic combinatorics etc. </subtitle><entry><title type="html">Topics in algebraic geometry codes</title><link href="https://galobelwang.github.io/blog/2025/topics-in-algebraic-geometry-code/" rel="alternate" type="text/html" title="Topics in algebraic geometry codes"/><published>2025-09-07T00:00:00+00:00</published><updated>2025-09-07T00:00:00+00:00</updated><id>https://galobelwang.github.io/blog/2025/topics-in-algebraic-geometry-code</id><content type="html" xml:base="https://galobelwang.github.io/blog/2025/topics-in-algebraic-geometry-code/"><![CDATA[]]></content><author><name>Galobel Wang</name></author><category term="geometry"/><summary type="html"><![CDATA[Some relative research of algebraic geometry codes]]></summary></entry><entry><title type="html">A brief introduction to differential geometry</title><link href="https://galobelwang.github.io/blog/2025/differential-geometry/" rel="alternate" type="text/html" title="A brief introduction to differential geometry"/><published>2025-08-28T00:00:00+00:00</published><updated>2025-08-28T00:00:00+00:00</updated><id>https://galobelwang.github.io/blog/2025/differential-geometry</id><content type="html" xml:base="https://galobelwang.github.io/blog/2025/differential-geometry/"><![CDATA[<h1 id="smooth-manifolds-and-tangent-spaces">Smooth Manifolds and Tangent Spaces</h1> <h3 id="introduction">Introduction</h3> <p>Here is a brief note on Differential Geometry for Information Geometry! In this post, let’s explore the fundamental concept of a <strong>smooth manifold</strong>. Think of manifolds as generalized surfaces – spaces that locally “look like” familiar Euclidean space (e.g., \(\mathbb{R}^n\)) but can have a more complex global structure.</p> <p>Why are manifolds important in machine learning?</p> <ul> <li>The <strong>parameter space</strong> of many machine learning models (like neural networks) can be viewed as a high-dimensional manifold.</li> <li>The <strong>loss landscape</strong>, which we navigate during optimization, is often a function defined over such a manifold.</li> <li>Some models have <strong>constraints</strong> on their parameters (e.g., weights of an autoencoder forming a low-dimensional representation, orthogonal matrices in certain RNNs) which naturally define manifolds.</li> </ul> <p>Our goal here is to build intuition for what manifolds are and introduce <strong>tangent spaces</strong>, which are crucial for understanding concepts like gradients in these curved settings.</p> <h3 id="beyond-euclidean-space-the-need-for-manifolds">Beyond Euclidean Space: The Need for Manifolds</h3> <p>In basic calculus and linear algebra, we often work within Euclidean spaces like \(\mathbb{R}^2\) (the plane) or \(\mathbb{R}^3\) (3D space). These spaces are “flat” and have a global coordinate system. However, many interesting spaces are not globally flat.</p> <p>Consider the surface of a sphere, \(S^2\). Locally, any small patch on the sphere looks like a piece of the flat plane \(\mathbb{R}^2\). But globally, you can’t map the entire sphere to a single flat plane without distortion (think of world maps). The sphere is a simple example of a manifold.</p> <p>In machine learning:</p> <ul> <li>The set of all probability distributions of a certain type (e.g., all Gaussian distributions) forms a manifold. The parameters (mean and covariance) live in this space.</li> <li>The set of weight matrices for a neural network layer, perhaps with some normalization constraints (e.g., weights on a sphere, orthogonal matrices), can form a manifold.</li> <li>The loss function of a neural network is a function \(L: \mathcal{W} \to \mathbb{R}\), where \(\mathcal{W}\) is the space of all possible weights. This space \(\mathcal{W}\) is typically a very high-dimensional manifold (often just \(\mathbb{R}^N\) for unconstrained networks, but its geometry under a suitable metric, like the Fisher Information Metric, can be non-trivial).</li> </ul> <p>Differential geometry provides the tools to perform calculus on these more general spaces.</p> <h3 id="what-is-a-smooth-manifold">What is a Smooth Manifold?</h3> <p>Intuitively, an \(n\)-dimensional manifold is a space that, if you “zoom in” enough at any point, looks like an open subset of \(\mathbb{R}^n\).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> An <strong>\(n\)-dimensional topological manifold</strong> \(M\)</p> </div> <p>A topological space \(M\) is an \(n\)-dimensional topological manifold if:</p> <ol> <li>\(M\) is <strong>Hausdorff</strong>: Any two distinct points have disjoint open neighborhoods.</li> <li>\(M\) is <strong>second-countable</strong>: \(M\) has a countable basis for its topology. (These ensure \(M\) is “nice” enough.)</li> <li>\(M\) is <strong>locally Euclidean of dimension \(n\)</strong>: Every point \(p \in M\) has an open neighborhood \(U\) that is homeomorphic to an open subset \(V \subseteq \mathbb{R}^n\). A homeomorphism is a continuous bijection with a continuous inverse.</li> </ol> <p>The pair \((U, \phi)\), where \(\phi: U \to V \subseteq \mathbb{R}^n\) is such a homeomorphism, is called a <strong>chart</strong> (or coordinate system) around \(p\). The functions \(x^i = \pi^i \circ \phi\) (where \(\pi^i\) are projections onto coordinate axes in \(\mathbb{R}^n\)) are local coordinate functions.</p> </blockquote> <details class="details-block"> <summary> <p><strong>Analogy.</strong> Chart on Earth</p> </summary> <p>Think of a map of a small region on Earth (e.g., a city map). This map is a chart. It represents a piece of the curved surface of the Earth on a flat piece of paper (\(\mathbb{R}^2\)). You need many such maps (an atlas) to cover the entire Earth, and where they overlap, they must be consistent.</p> </details> <p>For calculus, we need more than just a topological manifold; we need a <strong>smooth manifold</strong>. This means that when two charts overlap, the transition from one set of coordinates to another must be smooth (infinitely differentiable, \(C^\infty\)).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Smooth Atlas and Smooth Manifold</strong></p> </div> <p>Let \(M\) be an \(n\)-dimensional topological manifold.</p> <ol> <li> <p>Two charts \((U_\alpha, \phi_\alpha)\) and \((U_\beta, \phi_\beta)\) are <strong>smoothly compatible</strong> if \(U_\alpha \cap U_\beta = \emptyset\), or the <strong>transition map</strong></p> \[\psi_{\beta\alpha} = \phi_\beta \circ \phi_\alpha^{-1} : \phi_\alpha(U_\alpha \cap U_\beta) \to \phi_\beta(U_\alpha \cap U_\beta)\] <p>is a diffeomorphism (a smooth map with a smooth inverse). Both \(\phi_\alpha(U_\alpha \cap U_\beta)\) and \(\phi_\beta(U_\alpha \cap U_\beta)\) are open subsets of \(\mathbb{R}^n\).</p> </li> <li>An <strong>atlas</strong> for \(M\) is a collection of charts \(\mathcal{A} = \{(U_\alpha, \phi_\alpha)\}\) such that \(\bigcup_\alpha U_\alpha = M\).</li> <li>A <strong>smooth atlas</strong> is an atlas whose charts are all pairwise smoothly compatible.</li> <li>A <strong>smooth structure</strong> on \(M\) is a maximal smooth atlas (one that contains every chart compatible with it, and any two charts in it are smoothly compatible).</li> <li>A <strong>smooth manifold</strong> (or differentiable manifold) is a topological manifold equipped with a smooth structure.</li> </ol> </blockquote> <p>The key idea is that we can do calculus locally within each chart using standard multivariable calculus, and the smoothness of transition maps ensures that these local calculations are consistent across different charts.</p> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> The Circle \(S^1\)</p> </div> <p>The unit circle \(S^1 = \{(x, y) \in \mathbb{R}^2 \vert x^2 + y^2 = 1\}\) is a 1-dimensional manifold. We need at least two charts to cover \(S^1\). A common way is using angular parameterizations:</p> <ul> <li>Chart 1: Let \(U_1 = S^1 \setminus \{(1,0)\}\) (circle minus the point \((1,0)\)). Define \(\phi_1: U_1 \to (0, 2\pi)\) by \(\phi_1(\cos\theta, \sin\theta) = \theta\).</li> <li>Chart 2: Let \(U_2 = S^1 \setminus \{(-1,0)\}\) (circle minus the point \((-1,0)\)). Define \(\phi_2: U_2 \to (-\pi, \pi)\) by \(\phi_2(\cos\theta, \sin\theta) = \theta\).</li> </ul> <p>Consider the overlap. For instance, take the upper semi-circle, corresponding to \(\theta \in (0, \pi)\). Points here are in both \(U_1\) and \(U_2\). \(\phi_1(U_1 \cap U_2) = (0, \pi) \cup (\pi, 2\pi)\). \(\phi_2(U_1 \cap U_2) = (-\pi, 0) \cup (0, \pi)\).</p> <p>Let \(p \in U_1 \cap U_2\). If \(\phi_1(p) = \theta_1 \in (0, \pi)\), then \(\phi_2(p) = \theta_1\). The transition map \(\phi_2 \circ \phi_1^{-1}(\theta_1) = \theta_1\). If \(\phi_1(p) = \theta_1 \in (\pi, 2\pi)\) (e.g., lower semi-circle), then \(\phi_2(p) = \theta_1 - 2\pi\). The transition map \(\phi_2 \circ \phi_1^{-1}(\theta_1) = \theta_1 - 2\pi\). These transition functions are smooth (linear, in fact). (Another common way to define charts for spheres is using stereographic projection).</p> </blockquote> <p>Other examples of smooth manifolds:</p> <ul> <li>Any open subset of \(\mathbb{R}^n\) is an \(n\)-manifold (with a single chart: the identity map).</li> <li>The sphere \(S^n = \{x \in \mathbb{R}^{n+1} \mid \Vert x \Vert = 1\}\).</li> <li>The torus \(T^n = S^1 \times \dots \times S^1\) (\(n\) times).</li> <li>The space of \(m \times n\) matrices, \(\mathbb{R}^{m \times n}\), which is just Euclidean space.</li> <li><strong>Lie groups:</strong> Manifolds with a compatible group structure. Examples: <ul> <li>\(GL(n, \mathbb{R})\): invertible \(n \times n\) real matrices.</li> <li>\(O(n)\): orthogonal \(n \times n\) matrices (\(A^T A = I\)).</li> <li>\(SO(n)\): special orthogonal matrices (\(A^T A = I, \det A = 1\)).</li> <li>These are crucial in physics and also appear in ML (e.g., orthogonal RNNs, parameterizing rotations).</li> </ul> </li> </ul> <h5 id="smooth-functions-on-manifolds">Smooth Functions on Manifolds</h5> <p>A function \(f: M \to \mathbb{R}\) is <strong>smooth</strong> if for every chart \((U, \phi)\) on \(M\), the composite function \(f \circ \phi^{-1}: \phi(U) \to \mathbb{R}\) is smooth in the usual sense of multivariable calculus (i.e., it has continuous partial derivatives of all orders on the open set \(\phi(U) \subseteq \mathbb{R}^n\)). Similarly, a map \(F: M \to N\) between two smooth manifolds is smooth if its representation in local coordinates is smooth. Loss functions in ML are typically assumed to be smooth (or at least twice differentiable) on the parameter manifold.</p> <h3 id="tangent-vectors-and-tangent-spaces">Tangent Vectors and Tangent Spaces</h3> <p>Now that we have a notion of a smooth manifold, we want to do calculus. The first step is to define derivatives. On a manifold, derivatives are captured by <strong>tangent vectors</strong>.</p> <p>Intuitively, a tangent vector at a point \(p \in M\) is a vector that “points along” a curve passing through \(p\), representing the instantaneous velocity of the curve.</p> <p>There are several equivalent ways to define tangent vectors:</p> <h5 id="a-equivalence-classes-of-curves-intuitive">a) Equivalence Classes of Curves (Intuitive)</h5> <p>A smooth curve through \(p \in M\) is a smooth map \(\gamma: (-\epsilon, \epsilon) \to M\) such that \(\gamma(0) = p\). Two curves \(\gamma_1, \gamma_2\) through \(p\) are considered equivalent if their representations in any local chart \((U, \phi)\) around \(p\) have the same derivative (velocity vector in \(\mathbb{R}^n\)) at \(t=0\):</p> \[\frac{d}{dt}(\phi \circ \gamma_1)(t) \Big\vert_{t=0} = \frac{d}{dt}(\phi \circ \gamma_2)(t) \Big\vert_{t=0}\] <p>A tangent vector at \(p\) is an equivalence class of such curves. The set of all tangent vectors at \(p\) is the <strong>tangent space</strong> \(T_p M\). This space can be shown to have the structure of an \(n\)-dimensional vector space.</p> <h5 id="b-derivations-abstract-and-powerful">b) Derivations (Abstract and Powerful)</h5> <p>A <strong>derivation</strong> at a point \(p \in M\) is a linear map \(v: C^\infty(M) \to \mathbb{R}\) (where \(C^\infty(M)\) is the space of smooth real-valued functions on \(M\)) satisfying the Leibniz rule (product rule):</p> \[v(fg) = f(p)v(g) + g(p)v(f) \quad \text{for all } f, g \in C^\infty(M)\] <p>It can be shown that the set of all derivations at \(p\) forms an \(n\)-dimensional vector space, and this vector space is isomorphic to the tangent space defined via curves. This is often taken as the formal definition of \(T_p M\).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Tangent Space \(T_p M\)</strong></p> </div> <p>The <strong>tangent space</strong> to a smooth manifold \(M\) at a point \(p \in M\), denoted \(T_p M\), is the vector space of all derivations at \(p\). An element \(v \in T_p M\) is called a <strong>tangent vector</strong> at \(p\). If \(M\) is an \(n\)-dimensional manifold, then \(T_p M\) is an \(n\)-dimensional real vector space.</p> </blockquote> <p>Given a chart \((U, \phi)\) with local coordinates \((x^1, \dots, x^n)\) around \(p\), a natural basis for \(T_p M\) is given by the partial derivative operators with respect to these coordinates, evaluated at \(p\):</p> \[\left\{ \frac{\partial}{\partial x^1}\Big\vert_p, \dots, \frac{\partial}{\partial x^n}\Big\vert_p \right\}\] <p>Here, \(\frac{\partial}{\partial x^i}\Big\vert_p\) is the derivation that acts on a function \(f \in C^\infty(M)\) as:</p> \[\left(\frac{\partial}{\partial x^i}\Big\vert_p\right)(f) := \frac{\partial (f \circ \phi^{-1})}{\partial u^i} \Big\vert_{\phi(p)}\] <p>where \((u^1, \dots, u^n)\) are the standard coordinates on \(\mathbb{R}^n\) corresponding to \(\phi(U)\). Any tangent vector \(v \in T_p M\) can be written uniquely as a linear combination of these basis vectors:</p> \[v = \sum_{i=1}^n v^i \frac{\partial}{\partial x^i}\Big\vert_p\] <p>The coefficients \(v^i\) are the <strong>components</strong> of the vector \(v\) in the coordinate basis \(\{\partial/\partial x^i\vert_p\}\).</p> <blockquote class="box-info"> <p><strong>Connection to Gradients in ML.</strong> In Euclidean space \(\mathbb{R}^n\), the gradient \(\nabla f(p)\) of a function \(f\) at \(p\) is a vector. If we consider a path \(\gamma(t)\) with \(\gamma(0)=p\) and velocity \(\gamma'(0) = v\), then the directional derivative of \(f\) along \(v\) is \(D_v f(p) = \nabla f(p) \cdot v\). On a manifold, the concept analogous to the gradient is related to the <strong>differential</strong> \(df_p\) of a function \(f: M \to \mathbb{R}\). This differential \(df_p\) is an element of the <em>cotangent space</em> \(T_p^\ast M\) (the dual space of \(T_p M\)). It acts on tangent vectors: \(df_p(v) = v(f)\). If the manifold has a Riemannian metric (Part 2), there’s a natural way to identify tangent vectors with cotangent vectors. This allows us to define a <strong>gradient vector field</strong> \(\text{grad } f\) (or \(\nabla f\)) which is a tangent vector field. Its components in a local coordinate system are related to the partial derivatives \(\partial f / \partial x^i\). For now, think of tangent vectors as the “directions” in which one can move from \(p\), and \(T_p M\) is the space where these directions (and eventually gradients) live.</p> </blockquote> <h3 id="the-differential-pushforward-of-a-smooth-map">The Differential (Pushforward) of a Smooth Map</h3> <p>If we have a smooth map \(F: M \to N\) between two smooth manifolds, it induces a linear map between their tangent spaces at corresponding points.</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Differential (or Pushforward)</strong></p> </div> <p>Let \(F: M \to N\) be a smooth map between smooth manifolds. For any point \(p \in M\), the <strong>differential</strong> of \(F\) at \(p\) (also called the <strong>pushforward</strong> by \(F\) at \(p\)) is the linear map:</p> \[(F_\ast )_p : T_p M \to T_{F(p)} N\] <p>(also denoted \(dF_p\) or \(DF(p)\)) defined as follows: for any tangent vector \(v \in T_p M\) (viewed as a derivation) and any smooth function \(g \in C^\infty(N)\),</p> \[((F_\ast )_p v)(g) := v(g \circ F)\] <p>The function \(g \circ F\) is a smooth function on \(M\), so \(v(g \circ F)\) is well-defined. Alternatively, if \(v \in T_p M\) is represented by a curve \(\gamma: (-\epsilon, \epsilon) \to M\) with \(\gamma(0)=p\) and \(\gamma'(0)=v\), then \((F_\ast )_p v\) is the tangent vector at \(F(p) \in N\) represented by the curve \(F \circ \gamma: (-\epsilon, \epsilon) \to N\). That is, \((F_\ast )_p(\gamma'(0)) = (F \circ \gamma)'(0)\).</p> </blockquote> <p>In local coordinates, let \(M\) have coordinates \((x^1, \dots, x^m)\) near \(p\) and \(N\) have coordinates \((y^1, \dots, y^n)\) near \(F(p)\). If \(F\) is represented by coordinate functions \(y^j = F^j(x^1, \dots, x^m)\), then the matrix representation of \((F_\ast )_p\) with respect to the coordinate bases \(\{\partial/\partial x^i\vert_p\}\) and \(\{\partial/\partial y^j\vert_{F(p)}\}\) is the <strong>Jacobian matrix</strong> of \(F\) at \(p\):</p> \[[ (F_\ast )_p ]^j_i = \frac{\partial F^j}{\partial x^i} \Big\vert_p\] <p>So, if \(v = \sum_i v^i \frac{\partial}{\partial x^i}\Big\vert_p\), then \((F_\ast )_p v = w = \sum_j w^j \frac{\partial}{\partial y^j}\Big\vert_{F(p)}\), where</p> \[w^j = \sum_{i=1}^m \left( \frac{\partial F^j}{\partial x^i} \Big\vert_p \right) v^i\] <h3 id="vector-fields">Vector Fields</h3> <p>A <strong>smooth vector field</strong> \(X\) on a manifold \(M\) is a smooth assignment of a tangent vector \(X_p \in T_p M\) to each point \(p \in M\). “Smooth” here means that if we express \(X\) in any local coordinate system \((x^1, \dots, x^n)\) as</p> \[X(p) = \sum_{i=1}^n X^i(p) \frac{\partial}{\partial x^i}\Big\vert_p\] <p>then the component functions \(X^i: U \to \mathbb{R}\) are smooth functions on the chart’s domain \(U\). Equivalently, a vector field \(X\) is smooth if for every smooth function \(f \in C^\infty(M)\), the function \(p \mapsto X_p(f)\) (which can be written as \((Xf)(p)\)) is also a smooth function on \(M\).</p> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> Gradient Fields in Optimization</p> </div> <p>If \(M = \mathbb{R}^n\) (a trivial manifold), and \(L: \mathbb{R}^n \to \mathbb{R}\) is a smooth loss function, its gradient</p> \[\nabla L(p) = \left( \frac{\partial L}{\partial x^1}(p), \dots, \frac{\partial L}{\partial x^n}(p) \right)\] <p>is typically identified with the vector field</p> \[X_L(p) = \sum_{i=1}^n \frac{\partial L}{\partial x^i}(p) \frac{\partial}{\partial x^i}\Big\vert_p\] <p>Gradient descent involves taking steps in the direction of \(-X_L(p)\). More generally, on a Riemannian manifold (which we’ll introduce later), the gradient vector field \(\nabla L\) is intrinsically defined. Optimization algorithms often aim to follow trajectories of such (or related) vector fields to find minima of \(L\).</p> </blockquote> <h1 id="riemannian-metrics-and-geodesics">Riemannian Metrics and Geodesics</h1> <h3 id="introduction-1">Introduction</h3> <p>We have already introduced smooth manifolds as generalized spaces and tangent spaces as the local linear approximations where derivatives live. However, manifolds themselves don’t inherently come with a way to measure distances, angles, or volumes. To do this, we need to equip them with additional structure: a <strong>Riemannian metric</strong>.</p> <p>A Riemannian metric provides an inner product on each tangent space, varying smoothly from point to point. This is the key to unlocking a wealth of geometric notions:</p> <ul> <li>How long is a curve on the manifold?</li> <li>What is the shortest path (geodesic) between two points?</li> <li>What is the angle between two intersecting curves?</li> <li>How can we define volumes and integrate functions over manifolds?</li> </ul> <p>Understanding these concepts is crucial for appreciating how the “shape” of a parameter space influences optimization algorithms in machine learning.</p> <h3 id="riemannian-metrics-defining-local-geometry">Riemannian Metrics: Defining Local Geometry</h3> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Riemannian Metric</strong></p> </div> <p>A <strong>Riemannian metric</strong> \(g\) on a smooth manifold \(M\) is a smooth assignment of an inner product \(g_p: T_p M \times T_p M \to \mathbb{R}\) to each tangent space \(T_p M\). This means that for each \(p \in M\), \(g_p\) is a symmetric, positive-definite bilinear form on \(T_p M\). “Smooth assignment” means that if \(X, Y\) are smooth vector fields on \(M\), then the function \(p \mapsto g_p(X_p, Y_p)\) is a smooth function on \(M\). A smooth manifold \(M\) equipped with a Riemannian metric \(g\) is called a <strong>Riemannian manifold</strong>, denoted \((M, g)\).</p> </blockquote> <p>In local coordinates \((x^1, \dots, x^n)\) around a point \(p\), the metric \(g_p\) is completely determined by its values on the basis vectors \(\{\partial_i = \partial/\partial x^i\vert_p\}\):</p> \[g_{ij}(p) := g_p\left(\frac{\partial}{\partial x^i}\Big\vert_p, \frac{\partial}{\partial x^j}\Big\vert_p\right)\] <p>The functions \(g_{ij}(p)\) are the <strong>components of the metric tensor</strong> in these coordinates. They form a symmetric, positive-definite matrix \([g_{ij}(p)]\) for each \(p\). If \(v = v^i \partial_i\) and \(w = w^j \partial_j\) are two tangent vectors at \(p\) (using Einstein summation convention), their inner product is:</p> \[g_p(v, w) = \sum_{i,j=1}^n g_{ij}(p) v^i w^j\] <p>The length (or norm) of a tangent vector \(v\) is \(\Vert v \Vert_p = \sqrt{g_p(v,v)}\). The angle \(\theta\) between two non-zero tangent vectors \(v, w\) at \(p\) is defined by \(\cos \theta = \frac{g_p(v,w)}{\Vert v \Vert_p \Vert w \Vert_p}\).</p> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> Euclidean Metric on \(\mathbb{R}^n\)</p> </div> <p>On \(M = \mathbb{R}^n\) with standard Cartesian coordinates \((x^1, \dots, x^n)\), the standard Euclidean metric has \(g_{ij}(p) = \delta_{ij}\) (the Kronecker delta) for all \(p\). So, \(g_p(v,w) = \sum_{i=1}^n v^i w^i = v \cdot w\) (the usual dot product).</p> </blockquote> <blockquote class="box-example"> <div class="title"> <p><strong>Example.</strong> Metric on the Sphere \(S^2\)</p> </div> <p>The sphere \(S^2\) can be parameterized by spherical coordinates \((\theta, \phi)\). The metric induced from the standard Euclidean metric in \(\mathbb{R}^3\) is (for radius \(R=1\)):</p> \[(g_{ij}) = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \sin^2\theta \end{pmatrix}\] <p>So \(ds^2 = d\theta^2 + \sin^2\theta \, d\phi^2\). This is non-Euclidean; the components \(g_{ij}\) are not constant.</p> </blockquote> <h3 id="arc-length-distance-and-volume">Arc Length, Distance, and Volume</h3> <p>With a Riemannian metric, we can define:</p> <ul> <li> <p><strong>Length of a Curve:</strong> If \(\gamma: [a,b] \to M\) is a smooth curve, its length is</p> \[L(\gamma) = \int_a^b \Vert \gamma'(t) \Vert_{\gamma(t)} \, dt = \int_a^b \sqrt{g_{\gamma(t)}(\gamma'(t), \gamma'(t))} \, dt\] <p>In local coordinates \(x^i(t) = (\phi \circ \gamma)^i(t)\), this becomes</p> \[L(\gamma) = \int_a^b \sqrt{\sum_{i,j} g_{ij}(x(t)) \frac{dx^i}{dt} \frac{dx^j}{dt}} \, dt\] <p>The infinitesimal arc length element is often written as \(ds^2 = \sum_{i,j} g_{ij} dx^i dx^j\).</p> </li> <li> <p><strong>Distance (Riemannian Distance):</strong> The distance \(d(p,q)\) between two points \(p, q \in M\) is the infimum of the lengths of all piecewise smooth curves connecting \(p\) to \(q\):</p> \[d(p,q) = \inf \{ L(\gamma) \mid \gamma \text{ is a piecewise smooth curve from } p \text{ to } q \}\] </li> <li> <p><strong>Volume Form and Integration:</strong> On an oriented \(n\)-dimensional Riemannian manifold \((M,g)\), there’s a natural <strong>volume form</strong> (an \(n\)-form) \(\text{vol}_g\). In local oriented coordinates \((x^1, \dots, x^n)\), it’s given by:</p> \[\text{vol}_g = \sqrt{\det(g_{ij})} \, dx^1 \wedge \dots \wedge dx^n\] <p>This allows us to integrate functions \(f: M \to \mathbb{R}\) over \(M\):</p> \[\int_M f \, \text{vol}_g = \int_{\phi(U)} (f \circ \phi^{-1})(x) \sqrt{\det(g_{ij}(x))} \, dx^1 \dots dx^n\] <p>(using a partition of unity for global integration).</p> </li> </ul> <blockquote class="box-info"> <div class="title"> <p><strong>A Note on Metrics in Machine Learning: The Fisher Information Metric</strong></p> </div> <p>While we are discussing general Riemannian metrics, it’s worth noting a particularly important one in statistics and machine learning: the <strong>Fisher Information Metric (FIM)</strong>. If our manifold \(M\) is a space of probability distributions \(p(x; \theta)\) parameterized by \(\theta = (\theta^1, \dots, \theta^n)\), the FIM provides a natural way to measure “distance” or “distinguishability” between nearby distributions. Its components are given by:</p> \[(g_{ij})_{\text{Fisher}}(\theta) = E_{p(x;\theta)}\left[ \frac{\partial \log p(x;\theta)}{\partial \theta^i} \frac{\partial \log p(x;\theta)}{\partial \theta^j} \right]\] <p>The FIM captures the sensitivity of the distribution to changes in its parameters. Optimization algorithms that use the FIM (like Natural Gradient Descent) often exhibit better convergence properties by taking into account the geometry of this parameter space.</p> <p>We will <em>not</em> delve into the details or derivations of the FIM here. It serves as a prime example of how Riemannian geometry finds deep applications in ML. The FIM and its consequences will be the central topic of the <strong>Information Geometry crash course</strong>.</p> </blockquote> <h3 id="geodesics-straightest-paths">Geodesics: “Straightest” Paths</h3> <p>In Euclidean space, the shortest path between two points is a straight line. On a curved manifold, the concept of a “straight line” is replaced by a <strong>geodesic</strong>.</p> <p>Intuitively, a geodesic is a curve that is locally distance-minimizing. More formally:</p> <ul> <li>A curve \(\gamma(t)\) is a geodesic if its tangent vector \(\gamma'(t)\) is “parallel transported” along itself. (We’ll formalize parallel transport in Part 3 with connections).</li> <li>Equivalently, geodesics are critical points of the <strong>energy functional</strong> \(E(\gamma) = \frac{1}{2} \int_a^b g(\gamma'(t), \gamma'(t)) \, dt\). Curves that minimize length also minimize energy (if parameterized by arc length).</li> <li>Geodesics are curves with zero “acceleration” in the context of the manifold’s geometry.</li> </ul> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Geodesic Equation</strong></p> </div> <p>A curve \(\gamma(t)\) with local coordinates \(x^i(t)\) is a geodesic if it satisfies the geodesic equations:</p> \[\frac{d^2 x^k}{dt^2} + \sum_{i,j=1}^n \Gamma^k_{ij}(x(t)) \frac{dx^i}{dt} \frac{dx^j}{dt} = 0 \quad \text{for } k=1, \dots, n\] <p>Here, \(\Gamma^k_{ij}\) are the <strong>Christoffel symbols</strong> (of the second kind), which depend on the metric \(g_{ij}\) and its first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} \sum_{l=1}^n g^{kl} \left( \frac{\partial g_{jl}}{\partial x^i} + \frac{\partial g_{il}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^l} \right)\] <p>where \([g^{kl}]\) is the inverse matrix of \([g_{kl}]\). (We will formally introduce Christoffel symbols as components of a connection in Part 3).</p> </blockquote> <blockquote class="box-example"> <div class="title"> <p><strong>Examples of Geodesics</strong></p> </div> <ul> <li>On \(\mathbb{R}^n\) with the Euclidean metric, \(g_{ij} = \delta_{ij}\), so all \(\Gamma^k_{ij} = 0\). The geodesic equations become \(\frac{d^2 x^k}{dt^2} = 0\), whose solutions are straight lines \(x^k(t) = a^k t + b^k\).</li> <li>On the sphere \(S^2\), geodesics are great circles (e.g., lines of longitude, the equator).</li> <li>On a cylinder, geodesics are helices, circles, and straight lines along the axis.</li> </ul> </blockquote> <p><strong>Existence and Uniqueness:</strong> For any point \(p \in M\) and any tangent vector \(v \in T_p M\), there exists a unique geodesic \(\gamma_v(t)\) defined on some interval \((-\epsilon, \epsilon)\) such that \(\gamma_v(0)=p\) and \(\gamma_v'(0)=v\).</p> <p><strong>Exponential Map:</strong> The <strong>exponential map</strong> at \(p\), denoted \(\exp_p: T_p M \to M\), is defined by \(\exp_p(v) = \gamma_v(1)\). It maps a tangent vector \(v\) (thought of as an initial velocity) to the point reached by following the geodesic starting at \(p\) with velocity \(v\) for unit time. This map is a local diffeomorphism near the origin of \(T_p M\).</p> <blockquote class="box-tip"> <summary> <p><strong>Geodesics and Optimization</strong></p> </summary> <p>In optimization, we often think of gradient descent as following the “steepest descent” direction. If the parameter space has a non-Euclidean Riemannian metric (like the FIM), the “straightest path” for an optimization update might not be a straight line in the coordinate representation but rather a geodesic of this metric.</p> <ul> <li><strong>Gradient Flow:</strong> The continuous version of gradient descent, \(\frac{dx}{dt} = -\nabla_g L(x)\), describes curves whose tangent is the negative gradient vector field (with respect to the metric \(g\)). Understanding geodesics helps understand the behavior of such flows.</li> <li>Some optimization methods (like trust-region methods on manifolds) explicitly try to take steps along geodesics.</li> </ul> <p>&lt;/details&gt;</p> </blockquote> <h1 id="connections-covariant-derivatives-and-curvature">Connections, Covariant Derivatives, and Curvature</h1> <h3 id="introduction-2">Introduction</h3> <p>In the previous parts, we established smooth manifolds as our geometric spaces (Part 1) and endowed them with Riemannian metrics to measure lengths and angles. Now, we need tools to understand how geometric objects, particularly vector fields, <em>change</em> as we move across the manifold.</p> <ul> <li>How do we differentiate a vector field in a way that is intrinsic to the manifold, not dependent on a specific embedding in a higher-dimensional Euclidean space?</li> <li>How can we compare tangent vectors at different points? This leads to the idea of <strong>parallel transport</strong>.</li> <li>How do we quantify the “bending” or <strong>curvature</strong> of a manifold?</li> </ul> <p>These concepts are captured by <strong>connections</strong>, <strong>covariant derivatives</strong>, and the <strong>Riemann curvature tensor</strong>. They are vital for a deeper understanding of optimization on manifolds, as curvature, for instance, directly impacts the behavior of geodesics and the complexity of loss landscapes.</p> <h3 id="the-need-for-a-covariant-derivative">The Need for a Covariant Derivative</h3> <p>Consider a vector field \(Y\) on a manifold \(M\) and another vector field \(X\) (or a curve \(\gamma(t)\) with tangent \(X = \gamma'(t)\)). We want to define the derivative of \(Y\) in the “direction” of \(X\), denoted \(\nabla_X Y\). In \(\mathbb{R}^n\), if \(Y(x) = (Y^1(x), \dots, Y^n(x))\) and \(X_p = (X^1, \dots, X^n)\), the directional derivative is</p> \[(\nabla_X Y)(p) = \lim_{h \to 0} \frac{Y(p+hX_p) - Y(p)}{h} = \sum_j X^j \frac{\partial Y}{\partial x^j}(p)\] <p>Each component \((\nabla_X Y)^i = X(Y^i) = \sum_j X^j \frac{\partial Y^i}{\partial x^j}\). This simple component-wise differentiation doesn’t work directly on a general manifold because:</p> <ol> <li>\(Y(p+hX_p)\) is not well-defined: there’s no canonical “addition” on a manifold.</li> <li>Even if we use charts, \(Y(q) - Y(p)\) is not meaningful as \(T_q M\) and \(T_p M\) are different vector spaces.</li> <li>The basis vectors \(\partial/\partial x^i\) themselves change from point to point if the coordinates are curvilinear. Taking partial derivatives of components \(Y^j\) of \(Y = Y^j \partial_j\) does not capture this change in basis vectors.</li> </ol> <p>We need a derivative operator that produces a tangent vector and behaves like a derivative. This is an <strong>affine connection</strong>.</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Affine Connection (Covariant Derivative)</strong></p> </div> <p>An <strong>affine connection</strong> \(\nabla\) on a smooth manifold \(M\) is an operator</p> \[\nabla: \mathfrak{X}(M) \times \mathfrak{X}(M) \to \mathfrak{X}(M), \quad (X, Y) \mapsto \nabla_X Y\] <p>(where \(\mathfrak{X}(M)\) is the space of smooth vector fields on \(M\)) satisfying:</p> <ol> <li><strong>\(C^\infty(M)\)-linearity in \(X\):</strong> \(\nabla_{fX_1 + gX_2} Y = f \nabla_{X_1} Y + g \nabla_{X_2} Y\) for \(f,g \in C^\infty(M)\).</li> <li><strong>\(\mathbb{R}\)-linearity in \(Y\):</strong> \(\nabla_X (aY_1 + bY_2) = a \nabla_X Y_1 + b \nabla_X Y_2\) for \(a,b \in \mathbb{R}\).</li> <li><strong>Leibniz rule (product rule) in \(Y\):</strong> \(\nabla_X (fY) = (Xf)Y + f \nabla_X Y\) for \(f \in C^\infty(M)\).</li> </ol> <p>The vector field \(\nabla_X Y\) is called the <strong>covariant derivative</strong> of \(Y\) with respect to \(X\). If \(X_p \in T_pM\), then \((\nabla_X Y)_p\) depends only on \(X_p\) and the values of \(Y\) in a neighborhood of \(p\). So we can also define \(\nabla_v Y\) for \(v \in T_p M\).</p> </blockquote> <p>In local coordinates \((x^1, \dots, x^n)\) with basis vector fields \(\partial_i = \partial/\partial x^i\), the connection is determined by how it acts on these basis fields:</p> \[\nabla_{\partial_i} \partial_j = \sum_{k=1}^n \Gamma^k_{ij} \partial_k\] <p>The \(n^3\) functions \(\Gamma^k_{ij}\) are called the <strong>Christoffel symbols</strong> (or connection coefficients) of \(\nabla\). These are the same symbols that appeared in the geodesic equation in Part 2 if we use a specific connection. With these, the covariant derivative of \(Y = Y^j \partial_j\) along \(X = X^i \partial_i\) has components:</p> \[(\nabla_X Y)^k = X(Y^k) + \sum_{i,j=1}^n X^i Y^j \Gamma^k_{ij} = \sum_{i=1}^n X^i \left( \frac{\partial Y^k}{\partial x^i} + \sum_{j=1}^n Y^j \Gamma^k_{ij} \right)\] <p>The term \(\sum Y^j \Gamma^k_{ij}\) corrects for the change in the coordinate basis vectors.</p> <h3 id="the-levi-civita-connection">The Levi-Civita Connection</h3> <p>A general manifold can have many affine connections. If \((M,g)\) is a Riemannian manifold, there is a unique connection that is “compatible” with the metric and “symmetric”: the <strong>Levi-Civita connection</strong>.</p> <blockquote class="box-theorem"> <div class="title"> <p><strong>Theorem.</strong> <strong>Fundamental Theorem of Riemannian Geometry</strong></p> </div> <p>On any Riemannian manifold \((M,g)\), there exists a unique affine connection \(\nabla\) (called the <strong>Levi-Civita connection</strong> or Riemannian connection) satisfying:</p> <ol> <li> <p><strong>Metric compatibility:</strong> \(\nabla\) preserves the metric. That is, for any vector fields \(X, Y, Z\):</p> \[X(g(Y,Z)) = g(\nabla_X Y, Z) + g(Y, \nabla_X Z)\] <p>(This means the metric is “covariantly constant”: \(\nabla g = 0\)).</p> </li> <li> <p><strong>Torsion-free (Symmetry):</strong> For any vector fields \(X, Y\):</p> \[\nabla_X Y - \nabla_Y X = [X,Y]\] <p>where \([X,Y]\) is the Lie bracket of vector fields. In local coordinates, this is equivalent to \(\Gamma^k_{ij} = \Gamma^k_{ji}\) (symmetry of Christoffel symbols in lower indices).</p> </li> </ol> </blockquote> <p>The Christoffel symbols for the Levi-Civita connection are precisely those given in Part 2, derived from the metric \(g_{ij}\):</p> \[\Gamma^k_{ij} = \frac{1}{2} \sum_{l=1}^n g^{kl} \left( \frac{\partial g_{jl}}{\partial x^i} + \frac{\partial g_{il}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^l} \right)\] <p>From now on, unless stated otherwise, \(\nabla\) will refer to the Levi-Civita connection on a Riemannian manifold.</p> <h3 id="parallel-transport">Parallel Transport</h3> <p>The covariant derivative allows us to define what it means for a vector field to be “constant” along a curve. Let \(\gamma: I \to M\) be a smooth curve, and let \(V(t)\) be a vector field along \(\gamma\) (i.e., \(V(t) \in T_{\gamma(t)}M\) for each \(t \in I\)). The <strong>covariant derivative of \(V\) along \(\gamma\)</strong> is denoted \(\frac{DV}{dt}\) or \(\nabla_{\gamma'(t)} V\).</p> <blockquote class="box-definition"> <div class="title"> <p><strong>Definition.</strong> <strong>Parallel Transport</strong></p> </div> <p>A vector field \(V(t)\) along a curve \(\gamma(t)\) is said to be <strong>parallel transported</strong> along \(\gamma\) if its covariant derivative along \(\gamma\) is zero:</p> \[\frac{DV}{dt}(t) = \nabla_{\gamma'(t)} V(t) = 0 \quad \text{for all } t \in I\] </blockquote> <p>Given a vector \(v_0 \in T_{\gamma(t_0)}M\) at a point \(\gamma(t_0)\) on the curve, there exists a unique parallel vector field \(V(t)\) along \(\gamma\) such that \(V(t_0) = v_0\). This process defines a linear isomorphism, called <strong>parallel transport map</strong> \(P_{\gamma, t_0, t_1}: T_{\gamma(t_0)}M \to T_{\gamma(t_1)}M\), by \(P_{\gamma, t_0, t_1}(v_0) = V(t_1)\). If the connection is metric-compatible (like Levi-Civita), parallel transport preserves inner products, lengths, and angles: \(g(V(t), W(t)) = \text{const}\) if \(V,W\) are parallel along \(\gamma\).</p> <blockquote class="box-info"> <p><strong>Holonomy.</strong> If you parallel transport a vector around a closed loop \(\gamma\), it may not return to its original orientation. The difference measures the <strong>holonomy</strong> of the connection, which is related to curvature. On a flat space like \(\mathbb{R}^n\) with the standard connection, a vector always returns to itself. On a sphere, parallel transporting a vector around a latitude (other than the equator) will result in a rotated vector.</p> </blockquote> <p><strong>Geodesics Revisited:</strong> A curve \(\gamma(t)\) is a geodesic if and only if its tangent vector \(\gamma'(t)\) is parallel transported along itself: \(\nabla_{\gamma'(t)} \gamma'(t) = 0\). This is exactly the geodesic equation we saw earlier.</p> <h3 id="curvature-measuring-the-bending-of-a-manifold">Curvature: Measuring the “Bending” of a Manifold</h3> <p>Curvature quantifies how much the geometry of a Riemannian manifold deviates from being Euclidean (“flat”). A key way it manifests is that the result of parallel transporting a vector between two points can depend on the path taken.</p> <p>The <strong>Riemann curvature tensor</strong> (or Riemann tensor) \(R\) measures the non-commutativity of covariant derivatives, or equivalently, the failure of second covariant derivatives to commute. For vector fields \(X, Y, Z\), the Riemann tensor is defined as:</p> \[R(X,Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X,Y]} Z\] <p>It’s a \((1,3)\)-tensor, meaning it takes three vector fields and produces one vector field. Its components in a local coordinate system \((\partial_i = \partial/\partial x^i)\) are \(R^l_{ijk}\), where</p> \[R(\partial_i, \partial_j)\partial_k = \sum_l R^l_{ijk} \partial_l\] <p>Explicitly:</p> \[R^l_{ijk} = \frac{\partial \Gamma^l_{kj}}{\partial x^i} - \frac{\partial \Gamma^l_{ki}}{\partial x^j} + \sum_m (\Gamma^m_{kj} \Gamma^l_{im} - \Gamma^m_{ki} \Gamma^l_{jm})\] <p>A manifold is <strong>flat</strong> (locally isometric to Euclidean space) if and only if its Riemann curvature tensor is identically zero.</p> <p><strong>Symmetries of the Riemann Tensor:</strong> The Riemann tensor (in its \((0,4)\) form, \(R_{lijk} = g_{lm}R^m_{ijk}\)) has several symmetries:</p> <ol> <li> \[R_{lijk} = -R_{ljik}\] </li> <li>\(R_{lijk} = -R_{klij}\) (from \(R(X,Y)Z = -R(Y,X)Z\) and others)</li> <li>First Bianchi Identity: \(R(X,Y)Z + R(Y,Z)X + R(Z,X)Y = 0\) (or \(R_{lijk} + R_{ljki} + R_{lkij} = 0\))</li> </ol> <p>These symmetries reduce the number of independent components. For an \(n\)-manifold, there are \(n^2(n^2-1)/12\) independent components.</p> <ul> <li>For \(n=2\) (surfaces), there is 1 independent component.</li> <li>For \(n=3\), there are 6 independent components.</li> <li>For \(n=4\), there are 20 independent components.</li> </ul> <h5 id="sectional-curvature">Sectional Curvature</h5> <p>For a 2D plane \(\sigma \subset T_p M\) spanned by orthonormal vectors \(u, v\), the <strong>sectional curvature</strong> \(K(\sigma)\) or \(K(u,v)\) is given by:</p> \[K(u,v) = g(R(u,v)v, u)\] <p>This measures the Gaussian curvature of the 2D surface formed by geodesics starting at \(p\) in directions within \(\sigma\). If all sectional curvatures are constant \(c\), the manifold is a <strong>space of constant curvature</strong>.</p> <ul> <li>\(c &gt; 0\): e.g., sphere (locally).</li> <li>\(c = 0\): e.g., Euclidean space.</li> <li>\(c &lt; 0\): e.g., hyperbolic space (locally).</li> </ul> <h5 id="ricci-curvature-and-scalar-curvature">Ricci Curvature and Scalar Curvature</h5> <p>By contracting the Riemann tensor, we get simpler curvature measures:</p> <ul> <li> <p><strong>Ricci Tensor (\((0,2)\)-tensor):</strong></p> <p>\(\text{Ric}(X,Y) = \sum_i g(R(E_i, X)Y, E_i) \quad \text{(trace over first and third index of } R^l_{ijk})\) In coordinates: \(R_{jk} = \sum_i R^i_{jik}\). The Ricci tensor measures how the volume of a small geodesic ball deviates from that of a Euclidean ball. It plays a key role in Einstein’s theory of general relativity.</p> </li> <li> <p>Scalar Curvature (\(0\)-tensor, i.e., a scalar function): \(S = \text{tr}_g(\text{Ric}) = \sum_i \text{Ric}(E_i, E_i)\) In coordinates: \(S = \sum_j g^{jk} R_{jk}\). It’s the “total” curvature at a point. For surfaces (\(n=2\)), \(S = 2K\), where \(K\) is the Gaussian curvature.</p> </li> </ul>]]></content><author><name>Galobel Wang</name></author><category term="geometry"/><summary type="html"><![CDATA[Foundational preliminaries of information geometry]]></summary></entry><entry><title type="html">Kolmogorov’s axiomatization of probability</title><link href="https://galobelwang.github.io/blog/2025/axiomatization-of-probability/" rel="alternate" type="text/html" title="Kolmogorov’s axiomatization of probability"/><published>2025-08-26T00:00:00+00:00</published><updated>2025-08-26T00:00:00+00:00</updated><id>https://galobelwang.github.io/blog/2025/axiomatization-of-probability</id><content type="html" xml:base="https://galobelwang.github.io/blog/2025/axiomatization-of-probability/"><![CDATA[<p>The history of probability is a complex and convoluted subject that has been treated at length in many books. I’ve collected here are just a few aspects of it. My aim was to contextualize Kolmogorov’s axiomatization of probability theory; namely, to motivate the introduction of the seemingly complex machinery of measure theory to students that are usually only familiar with very elementary accounts of probability.</p> <h2 id="origins">Origins</h2> <p>First, I’d like to talk a little bit about the history of probability to put certain ideas that you might already have about the subject in context. It’ll also help us understand better the role of Kolmogorov’s axiomatization. This won’t be a historically thorough account by any means.</p> <p>Although randomness is present in ancient philosophy (e.g. in Democritus), there was no mathematical treatment of probability in antiquity.</p> <p>The mathematical theory of randomness can be traced back to Cardano (Italy, 1501-1576), who was the first to attempt a systematic study of probability in connection with games of chance. But his work was only published posthumously.</p> <p>Cardano still mixes the mathematical concept of probability with the unscientific concept of luck (an external force responsible for the fluctuations of outcomes).</p> <p>Moreover, he rather unsuccessfully attempted to base his theory on the notion of “odds”, rather than probabilities.</p> \[\operatorname{Odds}(\text{event E}) = \frac{\#\text{ of outcomes favorable to } E}{\#\text{ of outcomes unfavorable to } E}.\] <p>Partially because Cardano’s work was not published, and also because the “odds” were not the good quantity for convenient computations, the historical origin of probability theory is attributed to the <a href="https://www.york.ac.uk/depts/maths/histstat/pascal.pdf">correspondence between Blaise Pascal and Pierre de Fermat</a> around 1654. They discussed the “problem of the division of a stake between two players whose game was interrupted before its close.”</p> <p>For them and their successors in the XVII and XVIII centuries, probability was understood as follows.</p> <blockquote> <p><strong>Principle of Indifference (sometimes called Laplace’s principle).</strong><br/> The probability of an event is the ratio of the number of cases favorable to it, to the number of all cases possible when nothing else leads us to expect that any one of these cases should occur more than any other, which renders them, for us, equally possible.<br/><br/> —Laplace, “Analytical Theory of Probability”, 1812</p> </blockquote> <p>In practice, they introduced a set $S$ of possible outcomes of a certain “experiment” (such as tossing a die), and an <em>event</em> $A$ is defined by a set of “favorable” outcomes, whose probability is \(\mathbb P(A)=|A|/|S|.\)</p> <p>Here’s an example of a “classical” problem, the likes of which you might have already seen. Two dice are thrown together; let $X$ be the sum of both outcomes. The problem is to show that $X=9$ is more probable than $X=10$. To solve it, one introduces the set $S= \lbrace 1,2,3,4,5,6 \rbrace^2=\lbrace (i,j)\mid 1\leq i,j\leq 6 \rbrace$ as a model of the possible outcomes; $S$ has has 36 elements. The favorable cases to $X=9$ are $(3,6),\, (6,3) \,(4,5)$ and $(5,4)$, hence $\mathbb P(X=9)=4/36$ according to the principle of indifference. Similarly, the favorable cases to $X=10$ are $(4,6)\,, (5,4)$ and $(5,5)$, hence $\mathbb P(X=10)=3/36$.</p> <p>We see in this example that the <em>events</em> “the sum of both outcomes is $9$” and “the sum of both outcomes is $10$” are represented by <em>subsets</em> of the set $S$ of possible outcomes. This change of perspective, from <em>outcomes to events</em>, plays a key role in the axiomatization of probability.</p> <h2 id="law-of-large-numbers-and-frequentism">Law of large numbers and frequentism</h2> <p>One can also attach a number to the occurrence of a certain event. One obtains in this way the notion of a <strong>random variable</strong>. For example, at least for rational $p=n/(n+m)$, one might realize \(X= \begin{cases} 1 &amp; \text{ with probability } p \\ 0 &amp; \text{ with probability } 1-p \end{cases}.\) under the paradigm of the principle of indifference: for instance, by filling an urn with $n$ white balls and $m$ red balls, all identical in shape, and assigning the value $1$ to $X$ whenever a white is drawn. (Today we say that $X$ is a random variable <em>with Bernoulli distribution</em> of parameter $p$, denoted $\operatorname{Ber}(p)$.)</p> <p>By introducing numbers this way, one can combine several random variables using arithmetic operations $+$, $-$, $/$, and $\cdot$.</p> <p>Very fundamental was also the notion of <em>independence</em> of random variables: $X$ and $Y$ are independent if for any possible values $x$ and $y$ that they may respectively take \(\mathbb P(X=x \text{ and } Y=y) = \mathbb P (X=x)\mathbb P(Y=y).\)</p> <p>In 1962, Jakob Bernoulli (1655-1705) proved a first version of the law of large numbers (published in 1713): if $(X_i)_{i=1}^\infty$ is a sequence of independent random variables, each with distribution $\operatorname{Ber}(p)$, then \(\frac{\sum_{i=1}^n X_n}{n} \to p \quad \text{as} \quad n\to \infty.\)</p> <p>This is a first example of an <strong>asymptotic law of randomness</strong>: a random quantity, $\frac{\sum_{i=1}^n X_n}{n}$, is shown to approximate a deterministic one, $p$, as $n\to \infty$. The central limit theory is another example of such a law. (<a href="https://www.youtube.com/watch?v=AwEaHCjgeXk">This video is a nice introduction to the central limit theorem.</a>)</p> <p>In fact, Bernoulli proved something more refined: that \(\mathbb P\left(\left| \frac{ \sum_{i=1}^n X_n}{n} - p\right| &gt;\epsilon \right) \leq b(n,\epsilon)\) for an explicit upper bound $b(n,\epsilon)$ that vanishes as $n\to \infty$. (Nowadays, we call this <em>convergence in probability</em>.)</p> <p>The principle of indifference is well adapted to problems that have some natural symmetry, such as those connected with games of chance. In turn, the law of large numbers led to a <em>frequentist</em> interpretation of probability, under which symmetry is no longer needed: the probability of an event could be <em>defined</em> as the limit of its relative frequency under many trials. Probabilistic ideas were progressively applied to situations without evident symmetries, in the context of lawsuits, insurance, and demographics (tables of natality and mortality).</p> <h2 id="statistical-mechanics">Statistical mechanics</h2> <p>The field of applications of probabilistic ideas was considerably expanded in the XIX century.</p> <p>Most notably, Ludwig Boltzmann (Austria, 1844-1906) proposed that thermodynamic macroscopic observables, such as the temperature and the entropy, could be explained from microscopic considerations, using a probabilistic description of the possible configurations of the molecules that compose a given substance.</p> <p>For this, it was necessary to take limits as the number of particles goes to infinity and/or to consider continuous models (particles distributed in Euclidean space).</p> <p>Boltzmann ideas were further developed by Josiah Williard Gibbs (USA, 1839-1903), who also coined the term <strong>statistical mechanics</strong>.</p> <p>For instance, Boltzmann proposed that the entropy $S$ of a system is given by \(S=k_B \log(\#\text{ microscopic configurations of the system}),\) when all the configurations $X$ are equiprobable and the argument of the logarithm is finite. Here $k_B$ denotes a universal constant (Boltzmann’s constant). More generally, Gibbs gave the formula \(S= - k_B \sum_{x\in X} p(x) \log p(x),\) where $p:X\to \mathbb R_{\geq 0}$ satisfies $\sum_{x\in X} p(x) = 1$.</p> <p>In actual systems, the number of configurations is a priori infinite. The formulas above only make sense on finite portions of the system, and one is forced to consider a limiting procedure.</p> <p>The simplest setting on which one can perform this kind of limiting operation is the <em>Ising model</em>:</p> <p>Consider a finite and discrete set $\Lambda \subset \mathbb Z^2$ of sites organized in a square array.</p> <p>Let $s_{x,y}\in \lbrace +1,-1 \rbrace$ be a “spin” associated to the site $(x,y)\in \Lambda$. Then $\vec s = \lbrace s_{x,y} \rbrace_{(x,y)\in \Lambda}$ is a possible configuration of the system.</p> <p>The energy of this configuration is given by the Hamiltonian: \(\mathcal{H}(\vec s, BC) = - \sum\limits_{(x,y)} s_{x,y} \left(s_{x+1,y} + s_{x,y+1} \right) - h \sum\limits_{(x,y)} s_{x,y}\) Because of the interactions between neighbors in the sum, the hamiltonian depends on some given boundary conditions $BC$. Remark that two neighboring spins that are equal decrease the energy.</p> <p>The Boltzmann-Gibb’s theory postulates that, in equilibrium, the configurations of the system are distributed according to the probability law \(\mathbb P(\vec s) \propto \exp \left(- \frac{\mathcal{H}(\vec s)}{k_B T} \right),\) where $T$ is the temperature of the system. This is called today a <em>Gibb’s state.</em></p> <p>It turns out that the ``typical’’ or more probable configurations are very different, depending on the value of $T$. If $T$ is much smaller that a certain <em>critical temperature</em> $T_c$, then the alignment tendency of the spins predominates and one sees big clusters with the same spin. Whereas if $T\gg T_c$, disorder predominates. A phase transition happens at $T_c$. In fact, this $T_c$ is only well defined in the limit where the diameter of $\Lambda$ goes to infinity and the boundary conditions become irrelevant.</p> <p><a href="https://rf.mokslasplius.lt/ising-model/">You can see some simulations of this phenomenon here.</a></p> <h2 id="hilberts-sixth-problem">Hilbert’s sixth problem</h2> <p>Hilbert, who was one of the most important mathematicians of his time, proposed in 1900 a list of the most important problems in mathematics, which still inspires mathematicians today. Some of them had to do with the axiomatization of physics under the model of mathematics. His sixth problem concerned statistical mechanics:</p> <blockquote> <p>“The investigations on the foundations of geometry suggest the problem: To treat in the same manner, by means of axioms, those physical sciences in which mathematics plays an important part; in the first rank are the theory of probabilities and mechanics.<br/> As to the axioms of the theory of probabilities, it seems to me desirable that their logical investigation should be accompanied by a rigorous and satisfactory development of the method of mean values in mathematical physics, and in particular in the kinetic theory of gases” <br/> […] Thus Boltzmann’s work on the principles of mechanics suggests the problem of developing mathematically the limiting processes, there merely indicated, which lead from the atomistic view to the laws of motion of continua.</p> </blockquote> <p>An axiomatization of the theory of probability was proposed by the russian mathematician Andrei Kolmogorov in the monograph <a href="https://altexploit.files.wordpress.com/2017/07/a-n-kolmogorov-foundations-of-the-theory-of-probability-chelsea-pub-co-1960.pdf">“Foundations of the Theory of Probability”</a>, first published in German (as <em>Grundbegriffe der Wahrsckeinlichkeitrecknung</em>) in 1933. In its preface, Kolmogorov says:</p> <blockquote> <p>[The axiomatization of probability] would have been a rather hopeless one before the introduction of Lebesgue’s theories of measure and integration. However, after Lebesgue’s publication of his investigations, the analogies between measure of a set and probability of an event, and between integral of a function and mathematical expectation of a random variable, became apparent. These analogies allowed of further extensions ; thus, for example, various properties of independent random variables were seen to be in complete analogy with the corresponding properties of orthogonal functions.</p> </blockquote> <p>Although Kolmogorov humbly claimed that the basic ideas were already known to the specialists, his was the first “complete exposition of the whole system”. (I advise you to read Kolmogorov’s monograph: it remains one of the clearest expositions of the foundations of the theory.)</p> <p>Lebesgue’s theory of measure, extended later by Fréchet, was based on set theory (itself axiomatized by Zermelo in 1908). The theory of sets arose from problems concerning limits in analysis and the quest for ``foundations’’ for mathematics based on formal, logical procedures (see the work by Frege, Cantor, Zermelo, Russell and Whitehead…). At the beginning of the XXth century, set theory was imposing itself as the foundational tool of mathematics. Hence Kolmogorov’s probability is intrinsically set-theoretic; this has not been revisited until very recently, in the attempt to develop a categorical approach (<a href="http:*tobiasfritz.science/2019/cps_workshop/schedule.html">see this workshop</a>; category-theorists also have in mind a completely new approach to the foundation of mathematics called <a href="https:*en.wikipedia.org/wiki/Univalent_foundation">Univalent foundations</a>).</p> <h2 id="axiomatization-in-the-elementary-finite-case">Axiomatization in the “elementary” (finite) case</h2> <p>Let $X$ denote an experiment with possible outcomes $E_X= \lbrace x_1 ,…., x_n \rbrace$. According to the principle of indifference, each outcome has probability $ 1/n$. Similarly, any subset $A$ of $E_X$ (an event) has probability $|A| / n$. “Elementary” probability theory deals with many examples of this kind, involving coins, cards, urns, etc.</p> <p>However, the principle of indifference has limitations: maybe <em>something</em> leads us to expect that one of the cases occurs more than the others. So more generally, one introduces a function of probabilities $p:E_X \to [0,1]$ such that $\sum_{x\in E_X} p(x) = 1$. This induces a probability for every subset $A$ of $E_X$ via the formula \(\mathbb P(A) = \sum_{x\in A} p(x).\) The equality $P(E_X)=1$ expresses that with certainty one of the possible outcomes is going to be observed.</p> <p>Where do these $p(x)$ come from? It depends on your philosophical views. For frequentists, $p(x)$ should be an asymptotic approximation to the relative frequency of the output $x$ if the experiment $X$ is repeated a large number of times; the existence of a limit of this relative frequencies is thus supposed. Some claim that $p(x)$ expresses some propensity of nature to give the output $x$. For bayesians, the numbers $p(x)$ are just a quantification of our beliefs (i.e. a reasonable expectation); so a Bayesian might start supposing that a coin is fair (a <em>prior</em> belief), and then update this belief after some measurements using Bayes’ theorem, getting a <em>posterior</em>. (Bayesianism is not necessarily subjective i.e. “influenced by personal feelings, tastes, or opinions.”, as some claim; the question is rather: what are the beliefs that an <em>idealized rational observer</em> of a situation can have about it, given limited information e.g. certain observations?). See <a href="https://plato.stanford.edu/archives/win2012/entries/probability-interpret/">Interpretations of probability</a> in <em>The Stanford Encyclopedia of Philosophy</em>.</p> <p>As we briefly mentioned above, for the purpose of generalizing this story to infinite sets $E$, it is convenient to reformulate the definition of probability in terms of events rather than outcomes. This is done in two stages, defining first an <em>algebra of sets (a.k.a. events)</em> and then a <em>probability measure</em>.</p> <p>Let $E$ be a set. A set $\mathfrak F$ of subsets of $E$ is called an <em>algebra of sets</em> if</p> <ul> <li>$\mathfrak F$ contains $E$;</li> <li>$\mathfrak F$ is closed under complementation: If $A$ belongs to $\mathfrak F$, its complement $\bar A$ too.</li> <li>$\mathfrak F$ is closed under binary unions: If $A$ and $B$ belong to $\mathfrak F$, their union $A\cup B$ too.</li> </ul> <p><strong>Exercise.</strong> Prove that $\mathfrak F$ is also closed under binary intersections, finite unions, and finite intersections.</p> <p><strong>Exercise/Remark:</strong> When $\mathfrak F$ is finite, one can restate the definition in purely algebraic terms, without reference to the set $E$. (In case you run out of ideas, <a href="https://en.wikipedia.org/wiki/Field_of_sets#Fields_of_sets_in_the_representation_theory_of_Boolean_algebras">the answer is here</a>.)</p> <p>A <em>finite probability space</em> is given by a set $E$ (of any cardinality), a <em>finite</em> algebra of sets $\mathfrak F$, and a function $P:\mathfrak F\to [0,\infty)$ (<em>probability measure</em>, finite case) such that $P(E)=1$ and \(P(A\cup B)= P(A) + P(B)\) whenever $A$ and $B$ have no elements in common. This last property is called <em>additivity</em>.</p> <p>The elements of $\mathfrak F$ are called <em>random events</em>, and $P(A)$ is called the <em>probability</em> of the event $A$.</p> <p>If we think about $P(A)$ in frequentist terms i.e. as a quotient $ N(A)/N$ where $N(A)$ are occurrences of the event $A$ among $N$ trials, we can justify the requirements of $0 \leq P(A) \leq 1$ and additivity of disjoint events, since $N(A\sqcup B) = N(A) + N(B)$ (the symbol $\sqcup $ is disjoint union), hence $P(A\sqcup B) = P(A) + P(B).$ But of course this is just an heuristic motivation for the axioms.</p> <p>When $E$ is a finite set $E = \lbrace x_1,…, x_n \rbrace $ and $\mathfrak F$ equals the set of all subsets of $E$, the function $P:\mathfrak F\to [0,1]$ is uniquely determined by $\lbrace P(x_i) \rbrace _{i=1}^n$ and one recovers the outcome-based description that we introduced above.</p> <p>There is a dictionary between the language of sets and that of events.</p> <ul> <li>If $A\cap B= \emptyset$, then the random events $A$ and $B$ are <em>incompatible</em> or <em>mutually exclusive</em>.</li> <li>If $X=A\cap B$, the event $X$ corresponds to the simultaneous occurrence of $A$ and $B$; if $X=A\cup B$, the event $X$ is the occurrence of $A$ or $B$.</li> <li>The complementary set $\bar A$ corresponds to the non-occurrence of $A$.</li> <li>$\emptyset$ is the impossible event; $E$ is certitude (contains all possible outputs).</li> <li>If $B\subset A$, then the occurrence of $B$ implies the occurrence of $A$.</li> </ul> <p><strong>Exercise.</strong> Prove the elementary formulas:</p> <ul> <li>$P(\bar A) = 1-P(A). $</li> <li>If $A_1,…,A_n$ are incompatible, then \(P(A_1) + \cdots + P(A_n) = 1.\)</li> </ul> <h2 id="infinite-families-of-events">Infinite families of events</h2> <p>At first sight, the previous definition should work just fine if one allows algebras of fields of arbitrary cardinality, and not just finite ones. But infinities always come with delicate unwanted problems.</p> <p>At least two types of problems arise. First, there is no consistent way of assigning a probability $P(A)$ to <em>all</em> subsets of an infinite set respecting additivity; one runs into paradoxes ultimately connected with the axiom of choice (beware, one cannot picture the wildest sets!). Second, one wants to guarantee that limits are ‘“well behaved”; as we saw, one of the main reasons for axiomatizing probability was to make formal the “limiting processes” used in statistical physics.</p> <p>An illustration of the first kind of problem <a href="https://proofwiki.org/wiki/Hausdorff_Paradox">was provided by Hausdorff in 1914</a>. He showed that, if one first removes certain countable set of points of $S^2$ (the 2-dimensional sphere in $\mathbb R^3$), the remainder $E$ can be divided into three disjoint subsets $A$, $B$ and $C$ such that $A$, $B$, $C$, and $B\cup C$ are all congruent (i.e. equivalent under a rotation of the sphere). Therefore it is not possible to have an additive probability on all subsets of $E$ that would assign the same probability to congruent sets (in accordance with our geometrical intuitions), since $B\cup C$ should have probability $1/3$, $1/2$ and $2/3$ at the same time.</p> <p>Concerning limits, one requires a language rich enough to talk about an arbitrarily large number of events (or of conditions imposed on the outcomes of an experiment). For instance, consider the set $E=\lbrace H,T \rbrace^{\mathbb N}$, which models the fact of throwing a coin infinitely many times (i.e. an arbitrarily large number of times); an element $\omega = (\omega_0,\omega_1, \omega_2,…)$ is a particular sequence of heads and tails obtained performing the experiment. Then the (intuitively very unlikely) event of getting only heads can be written as an countable intersection of simpler events of the form $\lbrace \omega_i = H \rbrace$. Hence it’s natural to require that the algebra of events $\mathcal F$ be also closed under countable unions (equivalently, intersections): we call these $\sigma$-algebras. Similarly, the probability measures $P$ should be $\sigma$-additive: given a sequence $A_1,A_2,…$ of mutually exclusive events, \(P(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i) := \lim_{N\to \infty} \sum_{i=1}^N P(A_i).\)</p> <p>A pair $(E, \mathfrak F)$ made of a set $E$ and a $\sigma$-algebra of subsets of $E$ is called a measurable space. A triple $(E, \mathfrak F, P)$ such that $(E, \mathfrak F)$ is a measurable space and $P:\mathfrak F\to [0,\infty)$ is a $\sigma$-additive function such that $P(E)=1$ is called a <em>probability space</em>.</p> <p>Kolmogorov provided an equivalent characterization, that we propose here as an exercise.</p> <p><strong>Exercise.</strong> Let $(E,\mathfrak F)$ be a measurable space and $P:\mathfrak F\to [0,1)$ an additive function. Prove that $\sigma$-additivity of $P$ is equivalent to the following property (Kolmogorov’s axiom VI): if $(B_i)_{i=1}^{\infty}$ is a sequence of elements of $\mathfrak F$ such that</p> <p>$B_1 \supset B_2 \supset B_3 \supset \cdots$ and $\bigcap_i B_i = \emptyset,$</p> <p>then $\lim_{k\to \infty} P(B_k) = 0$.</p> <p>If $\mathfrak F$ is finite, this axiom VI (or $\sigma$-additivity) adds nothing new. Kolmogorov remarks that, in the truly infinite case, many events “are generally merely ideal events to which nothing corresponds in the outside world.” However, “if reasoning which utilizes the probabilities of such ideal events leads us to a determination of the probability of an actual event, then, from an empirical point of view also, this determination will automatically fail to be contradictory.” There is no “empirical” motivation of axiom VI, but it is an arbitrary limitation that turn out to be very useful and connected with the intuitive ideas about probability.</p>]]></content><author><name>Galobel Wang</name></author><category term="education"/><summary type="html"><![CDATA[Where I attempt to provide historical context for Kolmogorov's axiomatization of probability.]]></summary></entry></feed>